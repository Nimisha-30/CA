# -*- coding: utf-8 -*-
"""PS3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HB4Dn6M5BZSNATe_UiIdpt-lkIyw8xot
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Problem 1

## (i)
"""

import pandas as pd
data = pd.read_csv("/content/drive/My Drive/9Sem/20XT96 MS Lab/Coimbatore,India 2023-01-01 to 2023-12-31.csv")
data.head()

# Convert 'datetime' column to datetime format and extract day of year
import datetime as dt
data['datetime'] = pd.to_datetime(data['datetime'])
data['day_of_year'] = data['datetime'].dt.dayofyear

# Extract temperature data and corresponding day of the year
X = data['day_of_year'].values.reshape(-1, 1)
y = data['temp'].values

"""### a."""

# Fit and predict linear regression model
from sklearn.linear_model import LinearRegression
linear_model = LinearRegression()
linear_model.fit(X, y)
y_pred_linear = linear_model.predict(X)

"""### b."""

# Fit and predict quadratic regression model
from sklearn.preprocessing import PolynomialFeatures
poly_features_2 = PolynomialFeatures(degree=2)
X_poly_2 = poly_features_2.fit_transform(X)
quadratic_model = LinearRegression()
quadratic_model.fit(X_poly_2, y)
y_pred_quadratic = quadratic_model.predict(X_poly_2)

"""### c."""

# Fit and predict cubic regression model
poly_features_3 = PolynomialFeatures(degree=3)
X_poly_3 = poly_features_3.fit_transform(X)
cubic_model = LinearRegression()
cubic_model.fit(X_poly_3, y)
y_pred_cubic = cubic_model.predict(X_poly_3)

"""### d."""

from sklearn.metrics import mean_squared_error
import numpy as np
# Calculate RMSE for each model
rmse_linear = np.sqrt(mean_squared_error(y, y_pred_linear))
rmse_quadratic = np.sqrt(mean_squared_error(y, y_pred_quadratic))
rmse_cubic = np.sqrt(mean_squared_error(y, y_pred_cubic))

from sklearn.metrics import mean_absolute_error
# Calculate MAE for each model
mae_linear = mean_absolute_error(y, y_pred_linear)
mae_quadratic = mean_absolute_error(y, y_pred_quadratic)
mae_cubic = mean_absolute_error(y, y_pred_cubic)

print(f"Linear Model - RMSE: {rmse_linear:.4f}, MAE: {mae_linear:.4f}")
print(f"Quadratic Model - RMSE: {rmse_quadratic:.4f}, MAE: {mae_quadratic:.4f}")
print(f"Cubic Model - RMSE: {rmse_cubic:.4f}, MAE: {mae_cubic:.4f}")

"""Interpretation:

Lower RMSE values indicate better model performance. It tells you, on average, how far the predictions are from the actual values, with the error measured in the same units as the output variable (in this case, degrees Celsius).

Lower MAE values indicate better model performance. It provides a straightforward interpretation of error by telling you, on average, how much the model's predictions deviate from the actual values.

### e.
"""

# Calculate R-squared for each model
from sklearn.metrics import r2_score
r2_linear = r2_score(y, y_pred_linear)
r2_quadratic = r2_score(y, y_pred_quadratic)
r2_cubic = r2_score(y, y_pred_cubic)

print("R-squared values for:")
print("Linear Model:", r2_linear)
print("Quadratic Model:", r2_quadratic)
print("Cubic Model:", r2_cubic)

"""Interpretation:

The cubic model performs the best, explaining approximately 51.16% of the variance in temperature data.

The quadratic model also performs reasonably well, explaining around 39.74%.

The linear model explains very little of the variance (0.73%).

## (ii)
"""

# Calculate residuals for each model
residuals_linear = y - y_pred_linear
residuals_quadratic = y - y_pred_quadratic
residuals_cubic = y - y_pred_cubic

# Calculate mean and variance of residuals for each model
mean_residuals_linear = np.mean(residuals_linear)
variance_residuals_linear = np.var(residuals_linear)
mean_residuals_quadratic = np.mean(residuals_quadratic)
variance_residuals_quadratic = np.var(residuals_quadratic)
mean_residuals_cubic = np.mean(residuals_cubic)
variance_residuals_cubic = np.var(residuals_cubic)

print("Mean of residuals for:")
print("Linear Model:", mean_residuals_linear)
print("Quadratic Model:", mean_residuals_quadratic)
print("Cubic Model:", mean_residuals_cubic)
print("\nVariance of residuals for:")
print("Linear Model:", variance_residuals_linear)
print("Quadratic Model:", variance_residuals_quadratic)
print("Cubic Model:", variance_residuals_cubic)

"""Interpretation:

All models have residuals with a mean close to zero, as expected for a well-fitted model.

The variance of residuals decreases with the increasing degree of the polynomial, indicating better fit with higher-degree models.
"""

# Perform Shapiro-Wilk test for normality
import scipy.stats as stats
shapiro_linear = stats.shapiro(residuals_linear)
shapiro_quadratic = stats.shapiro(residuals_quadratic)
shapiro_cubic = stats.shapiro(residuals_cubic)

# Q-Q plots for visual inspection
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
stats.probplot(residuals_linear, dist="norm", plot=plt)
plt.title('Q-Q Plot: Linear Model Residuals')

plt.subplot(1, 3, 2)
stats.probplot(residuals_quadratic, dist="norm", plot=plt)
plt.title('Q-Q Plot: Quadratic Model Residuals')

plt.subplot(1, 3, 3)
stats.probplot(residuals_cubic, dist="norm", plot=plt)
plt.title('Q-Q Plot: Cubic Model Residuals')

plt.tight_layout()
plt.show()

print("Shapiro-Wilk Test p-value for:")
print("Linear Model:", shapiro_linear.pvalue)
print("Quadratic Model:", shapiro_quadratic.pvalue)
print("Cubic Model:", shapiro_cubic.pvalue)

"""Interpretation:

Mean and Variance: After running the code, compare the mean and variance of the residuals to check if the mean is close to 0 and variance is close to 𝜎^2.

Shapiro-Wilk Test: The p-value from the Shapiro-Wilk test tells us if the residuals are normally distributed. A p-value > 0.05 suggests normality.

Q-Q Plots: If the points on the Q-Q plot fall along the diagonal, it suggests that the residuals follow a normal distribution.

## (iii)
"""

# Generate 1000 random numbers for day of the year
random_days = np.random.randint(1, 366, size=1000).reshape(-1, 1)

# Predict temperature using the fitted models
random_days_poly_2 = poly_features_2.transform(random_days)
random_days_poly_3 = poly_features_3.transform(random_days)

temp_pred_linear = linear_model.predict(random_days)
temp_pred_quadratic = quadratic_model.predict(random_days_poly_2)
temp_pred_cubic = cubic_model.predict(random_days_poly_3)

# Interpolate using the original data
from scipy.interpolate import interp1d
interp_func = interp1d(X.flatten(), y, kind='linear', fill_value='extrapolate')
temp_interp = interp_func(random_days.flatten())

# Compare the results of the models with interpolation
comparison_df = pd.DataFrame({
    'Day': random_days.flatten(),
    'Linear_Pred': temp_pred_linear,
    'Quadratic_Pred': temp_pred_quadratic,
    'Cubic_Pred': temp_pred_cubic,
    'Interpolated': temp_interp
})

print("Comparison of the interpolation by various models:")
print(comparison_df.head())

"""## (iv)"""

# Predict temperature for the year 2024 (assuming linear extension of the data)
future_day_2024 = np.array([[365 + 365]])
future_day_poly_2 = poly_features_2.transform(future_day_2024)
future_day_poly_3 = poly_features_3.transform(future_day_2024)

temp_2024_linear = linear_model.predict(future_day_2024)
temp_2024_quadratic = quadratic_model.predict(future_day_poly_2)
temp_2024_cubic = cubic_model.predict(future_day_poly_3)

print("Temperature prediction for 2024 by various models:")
print("Linear Model:", temp_2024_linear[0])
print("Quadratic Model:", temp_2024_quadratic[0])
print("Cubic Model:", temp_2024_cubic[0])

"""# Problem 2"""

# Step 1: Load data and convert 'Date' to datetime format
axis_bank_data = pd.read_csv("/content/drive/My Drive/9Sem/20XT96 MS Lab/AXISBANK.NS.csv")
data.head()
axis_bank_data['Date'] = pd.to_datetime(axis_bank_data['Date'])
# Extract relevant columns
axis_bank_data = axis_bank_data[['Date', 'Open', 'Close']]

# Step 2: Generate 150 random working days in 2023 (considering only weekdays)
np.random.seed(42)  # For reproducibility
axis_bank_data['Day_of_Year'] = axis_bank_data['Date'].dt.dayofyear
# Filter for working days only
working_days = axis_bank_data[~axis_bank_data['Date'].dt.weekday.isin([5, 6])]  # Exclude weekends
random_working_days = working_days.sample(n=150, random_state=42)

# Step 3: Calculate the rate of return
random_working_days['Rate_of_Return'] = (random_working_days['Close'] - random_working_days['Open']) * 100 / random_working_days['Open']

# Step 4: Fit the data with linear, quadratic, and cubic regression models
X = random_working_days['Day_of_Year'].values.reshape(-1, 1)
y = random_working_days['Rate_of_Return'].values

# Linear regression
linear_model = LinearRegression()
linear_model.fit(X, y)
y_pred_linear = linear_model.predict(X)

# Quadratic regression
poly_features_2 = PolynomialFeatures(degree=2)
X_poly_2 = poly_features_2.fit_transform(X)
quadratic_model = LinearRegression()
quadratic_model.fit(X_poly_2, y)
y_pred_quadratic = quadratic_model.predict(X_poly_2)

# Cubic regression
poly_features_3 = PolynomialFeatures(degree=3)
X_poly_3 = poly_features_3.fit_transform(X)
cubic_model = LinearRegression()
cubic_model.fit(X_poly_3, y)
y_pred_cubic = cubic_model.predict(X_poly_3)

# Step 5: Compare and Validate the Fit using Statistical Tests
rmse_linear = np.sqrt(mean_squared_error(y, y_pred_linear))
rmse_quadratic = np.sqrt(mean_squared_error(y, y_pred_quadratic))
rmse_cubic = np.sqrt(mean_squared_error(y, y_pred_cubic))

mae_linear = mean_absolute_error(y, y_pred_linear)
mae_quadratic = mean_absolute_error(y, y_pred_quadratic)
mae_cubic = mean_absolute_error(y, y_pred_cubic)

print(f"Linear Model - RMSE: {rmse_linear:.4f}, MAE: {mae_linear:.4f}")
print(f"Quadratic Model - RMSE: {rmse_quadratic:.4f}, MAE: {mae_quadratic:.4f}")
print(f"Cubic Model - RMSE: {rmse_cubic:.4f}, MAE: {mae_cubic:.4f}")

# Step 6: Calcuate the coefficient of determination for each fit
r2_linear = r2_score(y, y_pred_linear)
r2_quadratic = r2_score(y, y_pred_quadratic)
r2_cubic = r2_score(y, y_pred_cubic)

print(f"Linear Model - R²: {r2_linear:.4f}")
print(f"Quadratic Model - R²: {r2_quadratic:.4f}")
print(f"Cubic Model - R²: {r2_cubic:.4f}")

# Step 7: Perform Error Analysis
from scipy.stats import shapiro, probplot

residuals_linear = y - y_pred_linear
residuals_quadratic = y - y_pred_quadratic
residuals_cubic = y - y_pred_cubic

mean_residuals_linear = np.mean(residuals_linear)
variance_residuals_linear = np.var(residuals_linear)

mean_residuals_quadratic = np.mean(residuals_quadratic)
variance_residuals_quadratic = np.var(residuals_quadratic)

mean_residuals_cubic = np.mean(residuals_cubic)
variance_residuals_cubic = np.var(residuals_cubic)

shapiro_linear = shapiro(residuals_linear)
shapiro_quadratic = shapiro(residuals_quadratic)
shapiro_cubic = shapiro(residuals_cubic)

# Print residual statistics and normality tests
print("Linear Model Residuals: Mean =", mean_residuals_linear, "Variance =", variance_residuals_linear)
print("Shapiro-Wilk Test p-value for Linear Model:", shapiro_linear.pvalue)

print("Quadratic Model Residuals: Mean =", mean_residuals_quadratic, "Variance =", variance_residuals_quadratic)
print("Shapiro-Wilk Test p-value for Quadratic Model:", shapiro_quadratic.pvalue)

print("Cubic Model Residuals: Mean =", mean_residuals_cubic, "Variance =", variance_residuals_cubic)
print("Shapiro-Wilk Test p-value for Cubic Model:", shapiro_cubic.pvalue)

# Step 8: Fit the values of errors with Linear, Quadratic, and Cubic regressions
# Linear regression on residuals
linear_res_model = LinearRegression()
linear_res_model.fit(X, residuals_linear)
res_pred_linear = linear_res_model.predict(X)

# Quadratic regression on residuals
quadratic_res_model = LinearRegression()
quadratic_res_model.fit(X_poly_2, residuals_quadratic)
res_pred_quadratic = quadratic_res_model.predict(X_poly_2)

# Cubic regression on residuals
cubic_res_model = LinearRegression()
cubic_res_model.fit(X_poly_3, residuals_cubic)
res_pred_cubic = cubic_res_model.predict(X_poly_3)

# Calculate RMSE for residual models
rmse_linear = np.sqrt(mean_squared_error(y, y_pred_linear))
rmse_quadratic = np.sqrt(mean_squared_error(y, y_pred_quadratic))
rmse_cubic = np.sqrt(mean_squared_error(y, y_pred_cubic))

print(f"Residuals Linear Model - RMSE: {rmse_linear:.4f}")
print(f"Residuals Quadratic Model - RMSE: {rmse_quadratic:.4f}")
print(f"Residuals Cubic Model - RMSE: {rmse_cubic:.4f}")

print()

# Calculate MAE for residual models
mae_linear = mean_absolute_error(y, y_pred_linear)
mae_quadratic = mean_absolute_error(y, y_pred_quadratic)
mae_cubic = mean_absolute_error(y, y_pred_cubic)

print(f"Residuals Linear Model - MAE: {mae_linear:.4f}")
print(f"Residuals Quadratic Model - MAE: {mae_quadratic:.4f}")
print(f"Residuals Cubic Model - MAE: {mae_cubic:.4f}")

print()

# Calculate R² for residual models
r2_res_linear = r2_score(residuals_linear, res_pred_linear)
r2_res_quadratic = r2_score(residuals_quadratic, res_pred_quadratic)
r2_res_cubic = r2_score(residuals_cubic, res_pred_cubic)

print(f"Residuals Linear Model - R²: {r2_res_linear:.4f}")
print(f"Residuals Quadratic Model - R²: {r2_res_quadratic:.4f}")
print(f"Residuals Cubic Model - R²: {r2_res_cubic:.4f}")

# Visualize the Q-Q plot for the residuals
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
probplot(residuals_linear, dist="norm", plot=plt)
plt.title('Q-Q Plot: Linear Model Residuals')

plt.subplot(1, 3, 2)
probplot(residuals_quadratic, dist="norm", plot=plt)
plt.title('Q-Q Plot: Quadratic Model Residuals')

plt.subplot(1, 3, 3)
probplot(residuals_cubic, dist="norm", plot=plt)
plt.title('Q-Q Plot: Cubic Model Residuals')

plt.tight_layout()
plt.show()

"""# Problem 3"""

# Step 1: Combine the models
# Using the models generated previously
# M1 = R_linear + E_linear
M1 = y_pred_linear + res_pred_linear
# M2 = R_linear + E_quadratic
M2 = y_pred_linear + res_pred_quadratic
# M3 = R_linear + E_cubic
M3 = y_pred_linear + res_pred_cubic
# M4 = R_quadratic + E_linear
M4 = y_pred_quadratic + res_pred_linear
# M5 = R_quadratic + E_quadratic
M5 = y_pred_quadratic + res_pred_quadratic
# M6 = R_quadratic + E_cubic
M6 = y_pred_quadratic + res_pred_cubic
# M7 = R_cubic + E_linear
M7 = y_pred_cubic + res_pred_linear
# M8 = R_cubic + E_quadratic
M8 = y_pred_cubic + res_pred_quadratic
# M9 = R_cubic + E_cubic
M9 = y_pred_cubic + res_pred_cubic

# Step 2: Validate the models with data from June 30, 2024
# Assuming you have data for June 30, 2024
# Predict the value for June 30, 2024 using the combined models
X_new = np.array([[181]])  # Example: 181st day corresponds to June 30

# Predictions for June 30, 2024 using the models
pred_M1 = linear_model.predict(X_new) + linear_res_model.predict(X_new)
pred_M2 = quadratic_model.predict(poly_features_2.transform(X_new)) + quadratic_res_model.predict(poly_features_2.transform(X_new))
pred_M3 = cubic_model.predict(poly_features_3.transform(X_new)) + cubic_res_model.predict(poly_features_3.transform(X_new))

# Compare with actual values (assuming you have them)
# actual_value = ...  # Value for June 30, 2024
print("Validation for June 30, 2024:")
print("Model M1 Prediction:", pred_M1)
print("Model M2 Prediction:", pred_M2)
print("Model M3 Prediction:", pred_M3)
# print("Actual Value:", actual_value)

# Step 3: Perform ANOVA test
from scipy.stats import f_oneway
# Flatten the residuals or errors for each model
errors_M1 = y - M1
errors_M2 = y - M2
errors_M3 = y - M3

# ANOVA Test: Null Hypothesis is that there's no significant difference between the models
f_statistic, p_value = f_oneway(errors_M1, errors_M2, errors_M3)

print(f"ANOVA F-statistic: {f_statistic}")
print(f"ANOVA p-value: {p_value}")

if p_value < 0.05:
    print("Reject the null hypothesis: Significant difference between models.")
else:
    print("Fail to reject the null hypothesis: No significant difference between models.")