# -*- coding: utf-8 -*-
"""IR Lab CA1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EoujlgPLta9ilL7WV4O7ZtcR1QKokH8F

## **Problem Sheet 1**
"""

import string
import nltk

stemmer = nltk.stem.PorterStemmer()

class Document:
    def __init__(self, id, title, content):
        self.ID = id
        self.title = title
        self.content = content
        self.process()

    def getID(self):
        return self.ID

    def getTitle(self):
        return self.title

    def process(self):
        self.preprocessed = preProcess(self.content)

    def display(self):
        print("ID:", self.ID)
        print("Title:", self.title)
        print("Content:")
        print(self.content)
        print("Preprocessed:")
        print(self.preprocessed)
        print("=======================")

def parse(content):
    documents = []
    docs = content.split('.I')[1:]
    for doc in docs:
        id = int(doc.split('\n')[0].strip())
        title = doc.split('.T')[1].split('.')[0].strip()
        content = doc.split('.W')[1].strip()
        documents.append(Document(id, title, content))
    return documents

def preProcess(content):
        content = removePunc(content)
        content = toLower(content)
        content = toTokens(content)
        content = stemming(content)
        return content

def removePunc(content):
    return content.translate(str.maketrans('', '', string.punctuation))

def toLower(content):
    return content.lower()

def toTokens(content):
    return content.split()

def stemming(content):
    return [stemmer.stem(word) for word in content]

with open('/content/drive/MyDrive/SEM-9/IR Lab/ADI.ALL') as file:
    content = file.read()

documents = parse(content)
uniqueTerms = set()

for doc in documents:
    for term in doc.preprocessed:
        uniqueTerms.add(term)

invertedIndex = {}

for term in uniqueTerms:
    invertedIndex[term] = set()
    for doc in documents:
        if term in doc.preprocessed:
            invertedIndex[term].add(doc.getID())

invertedIndex = dict(sorted(invertedIndex.items(), key = lambda item: len(item[1])))

stopWords = list(invertedIndex.keys())[-10:]
print("Stop Words:", stopWords)
for word in stopWords:
    del invertedIndex[word]

print(list(invertedIndex.keys()))

# docs = AND(invertedIndex['system'], invertedIndex['retriev'])

def processQuery(query):
    operators = {'AND': 2, 'OR': 1, 'NOT': 3, '(': 0, ')': 0}
    terms = query.split()
    stack = []
    postfix = []

    for term in terms:
        if term in operators:
            if term == '(':
                stack.append(term)
            elif term == ')':
                while stack and stack[-1] != '(':
                    postfix.append(stack.pop())
                stack.pop()
            else:
                while stack and stack[-1] != '(' and operators[stack[-1]] >= operators[term]:
                    postfix.append(stack.pop())
                stack.append(term)
        else:
            postfix.append(term)

    while stack:
        postfix.append(stack.pop())

    return postfix

processQuery('system AND retriev AND NOT ( pass OR ideal )')

def evaluateQuery(postfix):
    stack = []
    result = set()
    operators = {'AND': set.intersection, 'OR': set.union, 'NOT': lambda x: set(range(len(documents))) - x}

    for term in postfix:
        if term in operators:
            if term == 'NOT':
                operand = stack.pop()
                result = operators[term](operand)
            else:
                operand2 = stack.pop()
                operand1 = stack.pop()
                result = operators[term](operand1, operand2)
            stack.append(result)
        else:
            if term in invertedIndex:
                stack.append(invertedIndex[term])
            else:
                stack.append(set())

    return stack.pop()

evaluateQuery(processQuery('system AND retriev AND NOT ( pass OR ideal )'))

"""## **Problem Sheet 2**"""

dataset = {1: 'Information requirement: query considers the user feedback as information requirement to search.',
           2: 'Information retrieval: query depends on the model of information retrieval',
           3: 'Prediction problem: Many problems in information retrieval can be viewed as prediction problems',
           4: 'Search: A search engine is one of applications of information retrieval models.'}

newDocs = {5: 'Feedback: feedback is typically used by the system to modify the query and improve prediction',
           6: 'information retrieval: ranking in information retrieval algorithms depends on user query'}

import string
import nltk

nltk.download('stopwords')
stemmer = nltk.stem.PorterStemmer()

class DocHandler:
    def __init__(self, name, text):
        self.name = name
        self.text = text
        self.preProcess()

    def getShingles(self, k):
        self.shingles = set()
        for i in range(len(self.processed) - k + 1):
            shingle = tuple(self.processed[i: i + k])
            self.shingles.add(shingle)

    def display(self):
        print("Title:", self.name)
        print("Content:", self.text)
        print("-------------------------")

    def getName(self):
        return self.name

    def getText(self):
        return self.text

    def getPreProcessed(self):
        return self.processed

    def getFrequency(self):
        return self.freq

    def setTFIDF(self, tfidf):
        self.tfidf = tfidf

    def getTFIDF(self):
        return self.tfidf

    def preProcess(self):
        self.removePunc()
        self.toLower()
        self.toTokens()
        self.stopWordRemoval()
        self.stemming()
        self.frequency()

    def removePunc(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))

    def toLower(self):
        self.text = self.text.lower()

    def toTokens(self):
        self.tokens = self.text.split()

    def stopWordRemoval(self):
        stop_words = set(nltk.corpus.stopwords.words('english'))
        self.tokens = [word for word in self.tokens if word not in stop_words]

    def stemming(self):
        self.processed = [stemmer.stem(word) for word in self.tokens]

    def frequency(self):
        self.freq = {}
        for word in self.processed:
            if word in self.freq:
                self.freq[word] += 1
            else:
                self.freq[word] = 1

def titleCheck(content1, content2):
    title1 = content1.split(':')[0].lower()
    title2 = content2.split(':')[0].lower()
    return title1 == title2

def cosineSim(D, Q):
    numerator = 0
    for i in range(len(D)):
        numerator += D[i] * Q[i]
    denominatorD = 0
    for i in range(len(D)):
        denominatorD += D[i] * D[i]
    denominatorQ = 0
    for i in range(len(Q)):
        denominatorQ += Q[i] * Q[i]
    denominator = (denominatorD * denominatorQ) ** 0.5
    return numerator / denominator

import math

documents = []
queries = []

for d in dataset:
    documents.append(DocHandler(dataset[d].split(':')[0], dataset[d].split(':')[1][1:]))

for d in newDocs:
    queries.append(DocHandler(newDocs[d].split(':')[0], newDocs[d].split(':')[1][1:]))

N = len(documents)
uniqueTerms = []
for doc in documents:
    for term in doc.getPreProcessed():
        if term not in uniqueTerms:
            uniqueTerms.append(term)
df = {}
for term in uniqueTerms:
    df[term] = 0
    for doc in documents:
        if term in doc.getPreProcessed():
            df[term] += 1

tfidfMatrix = [[0 for i in range(len(uniqueTerms))] for j in range(N)]

for doc in documents:
    for term in uniqueTerms:
        if term in doc.getPreProcessed():
            tfidfMatrix[documents.index(doc)][uniqueTerms.index(term)] = (doc.getFrequency()[term] / len(doc.getPreProcessed())) * math.log10((N + 1) / (0.5 + df[term]))
        else:
            tfidfMatrix[documents.index(doc)][uniqueTerms.index(term)] = 0
    doc.setTFIDF(tfidfMatrix[documents.index(doc)])

qTFIDF = [[0 for i in range(len(uniqueTerms))] for j in range(len(queries))]
for q in queries:
    for term in uniqueTerms:
        if term in q.getPreProcessed():
            qTFIDF[queries.index(q)][uniqueTerms.index(term)] = (q.getFrequency()[term] / len(q.getPreProcessed())) * math.log10((N + 1) / (0.5 + df[term]))
        else:
            qTFIDF[queries.index(q)][uniqueTerms.index(term)] = 0
    q.setTFIDF(qTFIDF[queries.index(q)])

for q in queries:
    print("QUERY:", q.getName())
    for d in documents:
        sim = cosineSim(q.getTFIDF(), d.getTFIDF())
        print(d.getName(), '-', sim)
        if sim > 0.85:
            print("Duplicate!")
            break
        print('-------------------------------------------------')
    print('=================================================')

def jaccardSimilarity(set1, set2):
    intersection = set1.intersection(set2)
    union = set1.union(set2)
    return len(intersection) / len(union)

k = 2

for doc in documents:
    doc.getShingles(k)

for q in queries:
    q.getShingles(k)

for i in range(len(queries)):
    for j in range(len(documents)):
        similarity = jaccardSimilarity(queries[i].shingles, documents[j].shingles)
        print(f"Jaccard Similarity between '{queries[i].getName()}' and '{documents[j].getName()}': {similarity}")
        print()
    print()

import math

avgdl = 0
for doc in documents:
    avgdl += len(doc.getPreProcessed())
avgdl /= len(documents)

k1 = 1.2
b = 0.75

def OkapiBM25(D, Q):
    score = 0
    for term in Q:
        if term in df:
            idf = math.log10(((N - df[term] + 0.5) / (df[term] + 0.5)) + 1)
            tf = D.getFrequency().get(term, 0)
            score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len(D.getPreProcessed()) / avgdl))
    return score

for q in queries:
    print("QUERY:", q.getName())
    for d in documents:
        sim = OkapiBM25(d, q.getPreProcessed())
        print(d.getName(), '-', sim)
        if sim > 1:
            print("Duplicate!")
            break
        print('-------------------------------------------------')
    print('=================================================')