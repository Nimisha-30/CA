# -*- coding: utf-8 -*-
"""PS4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DGvsZbLDzXrU6SXzi1fwfYGPgKVXJtSK
"""

import string
import nltk

nltk.download('stopwords')
stemmer = nltk.stem.PorterStemmer()

class DocHandler:
    def __init__(self, name, text):
        self.name = name
        self.text = text
        self.preProcess()

    def getName(self):
        return self.name

    def getText(self):
        return self.text

    def getPreProcessed(self):
        return self.processed

    def getFrequency(self):
        return self.freq

    def setTFIDF(self, tfidf):
        self.tfidf = tfidf

    def getTFIDF(self):
        return self.tfidf

    def preProcess(self):
        self.removePunc()
        self.toLower()
        self.toTokens()
        self.stopWordRemoval()
        self.stemming()
        self.frequency()

    def removePunc(self):
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))

    def toLower(self):
        self.text = self.text.lower()

    def toTokens(self):
        self.tokens = self.text.split()

    def stopWordRemoval(self):
        stop_words = set(nltk.corpus.stopwords.words('english'))
        self.tokens = [word for word in self.tokens if word not in stop_words]

    def stemming(self):
        self.processed = [stemmer.stem(word) for word in self.tokens]

    def frequency(self):
        self.freq = {}
        for word in self.processed:
            if word in self.freq:
                self.freq[word] += 1
            else:
                self.freq[word] = 1

documents = {
    'd1': 'Pointer is going to go to the gym from today!',
    'd2': 'Kaamesh ate chicken biriyani last week, when he went out with his friends.',
    'd3': 'India might win a silver or gold medal, thanks to Vinesh Phogat!',
    'd4': 'This is the last sentence; I don\'t know what else to type',
    'd5': 'Kaamesh is good at studies and sports!'
}

docs = []
for doc in documents:
    docs.append(DocHandler(doc, documents[doc]))

pDocs = [obj.getPreProcessed() for obj in docs]

for doc in pDocs:
    print(doc)

print("Enter the query: ")
query = input()
query = DocHandler('Query', query)
pQuery = query.getPreProcessed()
print(pQuery)

def cosineSim(D, Q):
    numerator = 0
    for i in range(len(D)):
        numerator += D[i] * Q[i]
    denominatorD = 0
    for i in range(len(D)):
        denominatorD += D[i] * D[i]
    denominatorQ = 0
    for i in range(len(Q)):
        denominatorQ += Q[i] * Q[i]
    denominator = (denominatorD * denominatorQ) ** 0.5
    return numerator / denominator

uniqueTerms = []
for doc in pDocs:
    for term in doc:
        if term not in uniqueTerms:
            uniqueTerms.append(term)

N = len(docs)
df = {}
for term in uniqueTerms:
    df[term] = 0
    for pDoc in pDocs:
        if term in pDoc:
            df[term] += 1

import math

tfidfMatrix = []
for doc in docs:
    tfidfDoc = []
    maxFreq = max(doc.getFrequency().values())
    for term in uniqueTerms:
        if term in doc.getFrequency():
            tf = doc.getFrequency()[term] / maxFreq
            w = (1 + math.log10(tf)) * math.log10(N / df[term])
        else:
            tf = 0
            w = 0
        tfidfDoc.append(w)
    doc.setTFIDF(tfidfDoc)
    tfidfMatrix.append(tfidfDoc)

tfidfQuery = []
maxFreq = max(query.getFrequency().values())
for term in uniqueTerms:
    if term in query.getFrequency():
        tf = query.getFrequency()[term] / maxFreq
        w = (1 + math.log10(tf)) * math.log10(N / df[term])
    else:
        tf = 0
        w = 0
    tfidfQuery.append(w)
query.setTFIDF(tfidfQuery)

for doc in docs:
    print(doc.getName())
    print("Document:", doc.getText())
    print("Query:", query.getText())
    print(cosineSim(doc.getTFIDF(), query.getTFIDF()))
    print("================================")

"""#VSM"""

# Initialization and Unique Term Extraction
import math

uniqueTerms = []
for track in trackObjects:
    for term in track.getFrequency():
        if term not in uniqueTerms:
            uniqueTerms.append(term)

print("There are totally {} unique terms\n\n".format(len(uniqueTerms)))

# Document Frequency Calculation
N = len(track_list)
df = {}

for term in uniqueTerms:
    df[term] = 0
    for track in trackObjects:
        if term in track.getFrequency():
            df[term] += 1

# TF-IDF Calculation for Tracks
tfidfMatrix = []
for track in trackObjects:
    tfidfTrack = []
    maxFreq = max(track.getFrequency().values())
    for term in uniqueTerms:
        if term in track.getFrequency():
            tf = track.getFrequency()[term] / maxFreq
            w = (1 + math.log10(tf)) * math.log10(N / df[term])
        else:
            tf = 0
            w = 0
        tfidfTrack.append(w)
    track.setTFIDF(tfidfTrack)
    tfidfMatrix.append(tfidfTrack)

# TF-IDF Calculation for the Query
tfidfQuery = []
maxFreq = max(query.getFrequency().values())
for term in uniqueTerms:
    if term in query.getFrequency():
        tf = query.getFrequency()[term] / maxFreq
        w = (1 + math.log10(tf)) * math.log10(N / df[term])
    else:
        tf = 0
        w = 0
    tfidfQuery.append(w)
query.setTFIDF(tfidfQuery)

# Ranking Tracks by Cosine Similarity
sortedTracks = sorted(trackObjects, key=lambda x: cosineSim(x.getTFIDF(), query.getTFIDF()), reverse=True)