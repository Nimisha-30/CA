# -*- coding: utf-8 -*-
"""PS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwhomLlHlMdYFCWDM-0BjDs-4uAp1rq8
"""

# Document Database
D1 = "Information requirement : query considers the user feedback as information requirement to search ."
D2 = "Information retrieval : query depends on the model of information retrieval used ."
D3 = "Prediction problem : Many problems in information retrieval can be viewed as prediction problems ."
D4 = "Search : A search engine is one of applications of information retrieval models ."

# New Documents
D5 = "Feedback : feedback is typically used by the system to modify the query and improve prediction"
D6 = "information retrieval : ranking in information retrieval algorithms depends on user query"

# Extract the titles from the document
titles = []
titles_bin = []
docs = [D1,D2,D3,D4]
new_docs = [D5,D6]
for doc in docs:
  content = doc.split()
  temp =""
  i =0
  while content[i]!= ':':
    temp = temp + content[i] + " "
    i = i+1
  titles.append(temp.lower())
print(titles)

# Extract all the terms from the document
# terms will contain all the terms that are unique in the document
terms = []
for doc in docs:
  for term in doc.split():
    if term !=':' and term!='.' and term.lower() not in terms:
      terms.append(term.lower())
print(terms)

# Finding out the tf-idf matrix
import math
tfIdfMatrix = {}
for term in terms:
  tf =[]
  for doc in docs:
    len_i = len(doc.split())-2
    tf_ik = 0
    df_k =0
    for word in doc.split():
      if term.lower() == word.lower():
        tf_ik = tf_ik + 1
    for doc in docs:
      if term.lower() in doc.split():
        df_k = df_k +1
    tf.append(round((tf_ik/len_i)*math.log(5/(df_k+0.5)),2))
  tfIdfMatrix[term] = tf
print("The TF-IDF matrix is given by:")
print(tfIdfMatrix)

# Find out the binary tf idf matrix
tfIdf = {}
for term in terms:
  tf =[]
  for doc in docs:
    if term.lower() in doc:
      tf.append(1)
    else:
      tf.append(0)
  tfIdf[term.lower()] = tf
print(tfIdf)

# Check if the document is already present using the title
new_titles =[]
for doc in new_docs:
  content = doc.split()
  temp =""
  i =0
  while content[i]!= ':':
    temp = temp + content[i] + " "
    i = i+1
  new_titles.append(temp)
print(new_titles)
for title in new_titles:
  if title in titles:
    print("Duplicate document")
  else:
    print("New Document")

# Identify Duplicate Documents
new_tfIdf = []
for terms in tfIdfMatrix:
  tf = []
  for doc in new_docs:
    tf_ik = 0
    for word in doc.split():
      if terms.lower() == word.lower():
        tf_ik = tf_ik + 1
    tf.append(round((tf_ik/len(doc.split())-2)*math.log(5/(df_k+0.5)),2))
  new_tfIdf.append(tf)
print(new_tfIdf)

tf_idf_matrix = []
for terms in tfIdfMatrix:
  tf = []
  for doc in docs:
    tf_ik = 0
    for word in doc.split():
      if terms.lower() == word.lower():
        tf_ik = tf_ik + 1
    tf.append(round((tf_ik/len(doc.split())-2)*math.log(5/(df_k+0.5)),2))
  tf_idf_matrix.append(tf)
print(tf_idf_matrix)

import numpy as np
tf_idf_matrix = np.array(tf_idf_matrix)
new_tfIdf = np.array(new_tfIdf)

def identify_duplicate_doc():
  for i in range(len(new_docs)):
    new_doc_col  = new_tfIdf[:,i]
    for j in range(len(docs)):
        doc_col = tf_idf_matrix[:,j]
        dot_product = np.dot(new_doc_col,doc_col)
        if dot_product > 0.85:
            print("Duplicate document")
        else:
          print("New document")
identify_duplicate_doc()

pip install rank_bm25

# Using the Okapi library
from rank_bm25 import BM25Okapi
tokenized_corpus = [doc.split(" ") for doc in docs]
query = "information"
tokenized_query = query.split(" ")
bm25 = BM25Okapi(tokenized_corpus)
doc_scores = bm25.get_scores(tokenized_query)
print(doc_scores)